{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This excercise will help us to get started with using tools(pandas, sklearn etc) from python eco system.\n",
    "\n",
    "**Problem statement:** classify SMS messages as *HAM* or *SPAM* using **naive bayes** in supervised setting.\n",
    "See this link to get an idea supervised learning workflow [supervsed learning workflow](http://www.allprogrammingtutorials.com/tutorials/introduction-to-machine-learning.php)\n",
    "\n",
    "**Dataset:** We will use [SMS Spam Collection Data Set](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) from UCI machine learning repository.\n",
    "\n",
    "This notebook has taken text processing idea from\n",
    "https://radimrehurek.com/data_science_python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this notebook with anaconda installation requires installing Textblob libray for text processing\n",
    "\n",
    "Run following commands to install Textblob **from command prompt under anaconda**\n",
    "\n",
    "** conda install -c conda-forge textblob**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Must for inline plot\n",
    "%matplotlib inline \n",
    "import requests\n",
    "import pprint # for pretty printing\n",
    "import os # listing and managing file patho\n",
    "import zipfile # for zip and unzip utilities\n",
    "import pandas # for data analysis\n",
    "import csv\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer # for converting documents in word count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "r = requests.get(data_url)\n",
    "#r.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download and save the zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_zip_file = 'smsspamcollection.zip'\n",
    "#http = urllib3.PoolManager()\n",
    "with open(sms_zip_file, 'wb') as out_file:\n",
    "    out_file.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let verify it. See how you can run linux bash command using !\n",
    "**make sure output of following command contains smsspamcollection.zip file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let verify it. See how you can run linux bash command using !\n",
    "dir_listing = os.listdir('.') # list content of current directory\n",
    "print(dir_listing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 1: Can you complete following  code to check if sms_zip_file is present in above output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sms_zip_file ? , \"directory doesn't contain {}\".format(sms_zip_file) # hint look  in operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(sms_zip_file,\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's list the content of the new data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir('./data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMSSpamCollection file contains around 5k SMS messages. Checkour readme file for details.\n",
    "\n",
    "**Let's open this file and store line in python list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with  open('./data/SMSSpamCollection', 'r') as f:\n",
    "    sms_messages = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sms_messages[0:10]) # printing 10 messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is tab seperated(\\t) file with a new line in the end. Let remove new line. Note that label and actual message **ham, spam** is seperated by tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following code show how to write list cpmprehension. We could have done this using for loop too.\n",
    "# [<some_func>(x) for x in <something> if  <some_condition_is_true>]\n",
    "sms_messages = [m.rstrip() for m in sms_messages] # we are not using if condition part\n",
    "print('Number of sms messages is {}'.format(len(sms_messages)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's check couple of messages again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, msg in enumerate(sms_messages[0:20]): # see how we can slice list using : operator\n",
    "    print('message id {}  {}'.format(idx, msg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is our training data set $\\mathcal{D} = \\{({x_i}, y_i)\\}_{i=1}^{N=5574}$**. Using using this we will train(learn parameters $\\theta$ of a models(Naive bayes, Discriminant anlaysis based etc.)) and use trained model to classify new messages as ham or spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step before jumping into using any machine learning model is understanding the data by **describing it's statistical attribute and visualizating samples or sample property**.\n",
    "We can use CSV file reader and try to accomplish above task. But as they say python is a language with **battery(libraries) included**. Let's use **pandas and matplotlib** libraries to do this task as cleanly as possible. What to describe and what to plot will be an essential skill we build as we do various data science or machine learning tasks. Also with time you will also built a repositories of various packages available for different domain in python eco system. Most of the time reading blog and google search does the job of finding right libraries. Various packages for download and installation are avaiable at [PyPI - the Python Package Index](https://pypi.python.org/pypi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional**\n",
    "This is [10 Minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/10min.html)\n",
    "\n",
    "If you have more time look into this link [Pandas Tutorial: DataFrames in Python](https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python#gs.dEdNuDM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will see how wrapping the file in pandas simplify lot of tasks\n",
    "messages = pandas.read_csv('./data/SMSSpamCollection', sep='\\t', quoting=csv.QUOTE_NONE,\n",
    "                           names=[\"label\", \"message\"])\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try to understand various attribute of the data\n",
    "\n",
    "*How many messages in each group etc.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How long are each messages*. See how we can attach a new column(pandas Series) to the pandas object.\n",
    "\n",
    "It uses lambda(anonymous funtion) and map tell what to do with each entries in **message** column.\n",
    "One can write a python function and pass it there too like\n",
    "\n",
    "def get_length(msg):\n",
    "\n",
    "    return len(msg)\n",
    "\n",
    "**messages['length'] = messages['message'].map(get_length)**\n",
    "\n",
    "But we will take more pythonic route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "messages['length'] = messages['message'].map(lambda text: len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have length attribute.\n",
    "**To see the whole picture. Let plot length distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.length.plot(bins=20, kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there are enough messages of length upto 150 but very few messages are too long(>400).\n",
    "\n",
    "Let's try to summarize this distribution(hist) of length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.length.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infact there is a message  of length 910. What is this message?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Longest message is {}'.format(list(messages.message[messages.length > 900])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is there any difference in message length between spam and ham?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.hist(column='length', by='label', bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**looks like** on average spam messages has more length.\n",
    "\n",
    "**Q2:** Can you write the code to summarize above per class disribution of messages length(to be more precise about observation). i.e. can you group messages and describe their length?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computer only understand scalar or vector or matrices**. We need to convert text to vectors(feature).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll use the [Bag-of-words model](https://en.wikipedia.org/wiki/Bag-of-words_model) approach for creating feature\n",
    "representing our sms message.\n",
    "\n",
    "Bag of words is just feature genration idea where only count of words matter not the order. Later we will see\n",
    "there are better model for sentence or document representation where words order matters. There are model which takes into account the word order like [N-gram](https://en.wikipedia.org/wiki/N-gram) etc.\n",
    "Infact Deep learning has enabled us to learn better embedding of words using context of words(co occurance).\n",
    "We will try to use them in **deep learning section** [**optional** see [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to vector is bit involved and require a good understanding of NLP(natural language processing).\n",
    "\n",
    "But as we can imagine to convert a message into vector we need to\n",
    "1. convert a sentence into word token\n",
    "2. Normalize the words i.e do we care about(do they cary some infomration) Capital form(Cow vs cow), inflected form (\"goes\" vs. \"go\")\n",
    "3. Build a dictionary of words and map the messages into vector using this dictionary\n",
    "4. Finally train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Again we will use a python library [Textblob](http://textblob.readthedocs.io/en/dev/quickstart.html) to do heavy lifting for us.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write a function that will split a message into its individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_into_tokens(message):\n",
    "    message = unicode(message, 'utf8')  # convert bytes into proper unicode\n",
    "    return TextBlob(message).words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the original texts again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.message.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same messages, tokenized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.message.head().apply(split_into_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With textblob, normalize words into their base form [lemmas](https://en.wikipedia.org/wiki/Lemmatisation) with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_lemmas(message):\n",
    "    message = unicode(message, 'utf8').lower()\n",
    "    words = TextBlob(message).words\n",
    "    # for each word, take its \"base form\" = lemma \n",
    "    return [word.lemma for word in words]\n",
    "\n",
    "# see how head portion changes\n",
    "messages.message.head().apply(split_into_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use CountVectorizer from **sklearn** to convert each  message into **count vector**.   Any row of this matrix represents an example(count of various words in the message).\n",
    "\n",
    "**Let's create the transformation class first**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may have to uncomment following two line once if running following cell produces error\n",
    "#import nltk\n",
    "#nltk.download()\n",
    "bow_transformer = CountVectorizer(analyzer=split_into_lemmas).fit(messages['message'])\n",
    "print('Number of unique words in our dictionary are {}'.format(len(bow_transformer.vocabulary_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using this tranformer can can convert any message into count of words representation.\n",
    "\n",
    "**Let see how message 4 gets transformed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message4 = messages['message'][3]\n",
    "print(message4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's convert it and check the converted message(Bag of word representation) and its shape\n",
    "bow4 = bow_transformer.transform([message4])\n",
    "print(bow4)\n",
    "print(bow4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vector count representation of message 4 is of length 8859(size of our vocabulary) and we are only prnting the indices where count is not zero. \n",
    "So, nine unique words in message nr. 4, two of them appear twice, the rest only once. Sanity check: what are these words the appear twice in this message?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(message4)\n",
    "print(bow_transformer.get_feature_names()[6726])\n",
    "print (bow_transformer.get_feature_names()[8002])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let conver whole SMS corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_bow = bow_transformer.transform(messages['message'])\n",
    "print 'sparse matrix shape:', messages_bow.shape\n",
    "print 'number of non-zeros:', messages_bow.nnz\n",
    "print 'sparsity: %.2f%%' % (100.0 * messages_bow.nnz / (messages_bow.shape[0] * messages_bow.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the mode or estimating parameters $\\theta$ of the model\n",
    "Now we have vector feature representation $x_i$ of our sms samples. \n",
    "\n",
    "Let review some theory and see what paramters we need to estimate for Naive bayes model.\n",
    "\n",
    "We know that we classify a sms $x_i$  to a class c= HAM or c= SPAM which has maximum vlaue of $P(c|x_i).$ Using bayes rule we have $P(c|x_i) = \\frac{P(x_i|c) P(c)}{P(x_i)} \\propto P(x_i|c) P(c)$ as normalization doesn't depend on class label. \n",
    "\n",
    "In naive bayes assumption for modelling class conditional densities we have $P(x_i|c) = \\prod_j^D P(x_{ij}|c)$ assuming  $x_i \\in \\mathbb{R}^D$\n",
    "\n",
    "**Note:$D$ is size of our vacabulary ($|V|$) build from sms document corpus i.e D = |V|**\n",
    "\n",
    "**what probability distribution we should choose for $P(x_{i}|c)?$ **\n",
    "\n",
    "Each value $x_{ij}$ is an integer values and there are total $D$ different unique values(word). This definetly suits a $D$ side die situation. \n",
    "\n",
    "**Infact ham or spam document generation in bag of word model is nothing but rolling this die. Pick the word dictated by the side of die throw.**\n",
    "\n",
    "Now we  know that we can put multinomial distribution for such situation. Hence\n",
    "\n",
    "$P(x_i|C) = \\frac{n_i}{\\prod_j^D x_{ij|C}} \\prod^{D} P(w_j|c)^{x_{ij}} \\propto  \\prod^{D} P(w_j|c)^{x_{ij}}$ as normalization doesn't depend on class label\n",
    "\n",
    "We know that using MLE estimate we have $P(w_j|c) = \\frac{\\sum_{i=1}^N x_{ij}\\mathbb{1}(y_i=c)}{\\sum_{k=1}^{D} \\sum_{i=1}^N x_{ik}\\mathbb{1}(y_i=c)}.$ where $\\mathbb{1}$ is indicator function.\n",
    "\n",
    "This is nothing but relative frequency of $w_j$ in documents of class c=SPAM or c= HAM\n",
    "with respect to the total number of words in documents of that class.\n",
    "\n",
    "prior class  densites are estimated as $P(c) = \\frac{N_c}{N}.$ Where $N_c$ are numer of document in class k.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. Finish following function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_class_probability_of_words(messages_bow, messages_label, class_label):\n",
    "    '''This function estimates the parameter of mutlinomial distribution in BOF model in class_label\n",
    "    args:\n",
    "    messages_bow = BOW encoded messages\n",
    "    messages_label = label(ham or spam) if the messages.(Note this is supervised setting we need class label)\n",
    "    class_label = 'ham' or 'spam'\n",
    "    returns:\n",
    "    return the list of estimated parameter of size D\n",
    "    '''    \n",
    "    # write your code here. You can write code in multiple cell to write different part of function and finally\n",
    "    # merge them into on cell as shown in the class.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. Write a function with full signature of estimating prior class densities.\n",
    "# You function should return python list of size 2. First entry should be estimate for ham and second entry for spam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your function here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. Using above functions, predict the  class of training messages in messages_bow. return the list predicting 'ham' or 'spam' for messges?\n",
    "# use classification_report, f1_score, accuracy_score, confusion_matrix function form sklearn.metrics to show your results.\n",
    "\n",
    "see this blog  https://radimrehurek.com/data_science_python/ about how to use these function in cell 27, 28 and 29."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6 use  MultinomialNB function from sklearn on BOG model and use same metrices as in Q5. look from cell 24 to 28 in the blog notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
