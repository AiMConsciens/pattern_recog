*Pattern Recognition*
  - Course: ENCE 3630 and ENCE 4630 2017
  - Instructor: /Pooran Singh Negi, pooran.negi@gmail.com/
  - Office: 248
  - Office Hours: After the class T, Th,  3.50- 4.50. Email for 1-on-1 help.
* TextBooks
- *Required:*
  -  [[https://www.cs.ubc.ca/~murphyk/MLbook/][Machine Learning: a Probabilistic Perspective by Kevin Patrick Murphy]]. Online version is available in the [[https://library.du.edu/][DU library]].
  -  [[http://www-bcf.usc.edu/~gareth/ISL/][An Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani]]. It is available online in pdf format
- *Optional:* For more advance treatment [[https://web.stanford.edu/~hastie/ElemStatLearn/][The Elements of Statistical Learning]]. It is available online in pdf format   
* Course Description
Pattern recognition is an exciting area. We will go though theory behind
pattern recognition using tool from probability and linear algebra.
We will use python, its [[https://www.scipy.org/][scientific libraries]] (numpy, scipy, matplotlib etc.)
and [[http://scikit-learn.org/stable/][scikit-learn: Machine Learning in Python]] during the course. For deep neural network part, we will use
highly popular [[https://www.tensorflow.org/][tensorflow Machine Intelligence]] library from the Google. For assignments, starter code  or hint will be given.
One can use other languages and tools as per comfort level. 
At the end of the course one should be able to understand theory behind various
machine learning algorithms with a unifying perspective from probability theory, be comfortable using open source tools for building machine learning systems.

* Software
Please install [[https://www.anaconda.com/download/][Anaconda for Python 2.7 data science platform. ]]Please install it before coming in the class on Tuesday.
See the youtube link [[https://www.youtube.com/watch?v=OOFONKvaz0A][Installing Anaconda, Jupyter Notebook]]. 
You can also go to my  [[https://github.com/psnegi/PythonForReproducibleResearch][python for reproducible research]]  github repository and start by running pythonBasic.ipynb notebook.
I will go over basic of python and jupyter notebook. For Deep Neural networks, we will go over installation instruction later in the course.
* Prerequisite
Student should be comfortable with linear algebra, probability, statistics,
optimization and some programming experience. We will cover basics in the class.

* Syllabus
Here are the main topics for the class. More topics can be added as per class interest and available time.
- Basic idea of machine learning, and probability
- Generative models, parametric estimation and supervised learning.
  - Naive Bayes classifier etc.
- Gaussian models
- Linear and logistic regression
- Support vector machine, Kernels
- Decision tree.
- Bagging, boosting and model selection etc.
- Unsupervised learning
  - Clustering etc.
- Deep learning
  - Artificial Neural Networks(ANN), End to end learning, cost function
  - Convolutional Neural Networks(CNN) for classification(image) and regression
  - Recurrent Neural Networks for natural language processing(NLP) and time series data
* Grading
There will be one mid term, a final exam, homework assignments, in class quizzes.
Worst 2 of your homework assignments and 2 quizzes will be dropped. Students enrolled in 
ENCE 4630 will be asked to do slightly more homework questions.


|----------------------------+-------------------------+
| Homework + Quizzes         |             30(20+10) % |
|----------------------------+-------------------------+
| Midterm exam, 17th October |                     25% |
|----------------------------+-------------------------+
| Final exam, 21 Nov         |                     25% |
|----------------------------+-------------------------+
| Final Project, Due 23 Nov  |                     20% |
|----------------------------+-------------------------+

* Homework
Homework numbers are as per *Kevin Murphy ebook from the library*
|                                                                                                                                                | HW | Due Date |
|------------------------------------------------------------------------------------------------------------------------------------------------+----+----------|
| This homework is meant to dust off your probability hat.                                                                                       |    |          |
| Chapter 2, 2.1(use bayes rule, condition on event actually observed. like in part a say N_b = number of boys, N_b no of girls), 2.3, 2.6, 2.12 |  1 | sep 21   |
| 2.16(look for beta distribution definition and use gamma function definition)                                                                  |    |          |
|------------------------------------------------------------------------------------------------------------------------------------------------+----+----------|
| 3.6, 3.7, *3.11(only if enrolled in ENCE 4630)*, 3.20, 4.1(look into section 2.5.1 for definition of correlation coefficient), 4.14            |  2 | sep 28   |
|                                                                                                                                                |    |          |

* Course Lectures


| Date   | Reading assignment                                                                         | uploaded slides                                                                                                                                        |
|--------+--------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------|
| 12 sep | Read chapter 1 of Kevin Murphy                                                             | [[./lectures/lecture1.pdf][overview of pattern recognition]], [[http://cs229.stanford.edu/section/cs229-linalg.pdf][linear algebra overview]]                                                                                               |
|--------+--------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------|
| 14 sep | section 2.2, 2.3, 2.4[.1, .2, .3, .4, .5, .6], 2.5[.1, .2, .4], 2.6.1, 2.8 of kevin Murphy | overview of probability and information theory from the Murphy book + [[http://cs229.stanford.edu/section/cs229-prob.pdf][Review of Probability Theory]]                                                     |
|        |                                                                                            | Here is the link of  jupyter notebook created in the class [[./lectures/intro_notebook.ipynb][jupyter notebook introduction]]                                                               |
|--------+--------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------|
| 19 Sep | chapter 3 of Kevin Murphy                                                                  | [[./lectures/lecture3.pdf][slides]]                                                                                                                                                 |
|        |                                                                                            | [ *optional* [[https://metacademy.org/graphs/concepts/bayesian_parameter_estimation#lfocus%3Dbayesian_parameter_estimation][Bayesian parameter estimation]]]                                                                                                            |
|--------+--------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------|
| 21 Sep | k. M. book  4.1 upto 4.2.5                                                                 | We covered navie bayes and looked into mutlivariate gaussian(MVG). Modelling class conditional densities                                               |
|        | [optional] ISLR book from    4.4.1 upto 4.4.4.                                             | as  MVN lead to quadratic discriminant analysis(QDA) and linear discriminant analysis (LDA) when covariance matrices are tied i.e. $\Sigma_c = \Sigma$ |
|        |                                                                                            | Here is the [[https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/constrained-optimization/a/lagrange-multipliers-examples][link]] to mechanics of Lagrangian multiplier. For more detail see this link of [[https://metacademy.org/graphs/concepts/lagrange_duality#focus%3Dlagrange_multipliers&mode%3Dlearn][metacademy]]. Go over free section                              |
|--------+--------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------|
| 26 Sep | K. M. book 5.7 upto  5.7.2.2                                                               |                                                                                                                                                        |
